% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}
\usepackage{url}
\usepackage{amsmath}


\begin{document}

\title{Twitter-based Sentiment Analysis}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{5} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor Jakob Gruber\\
      0203440\\
       \email{}
% 2nd. author
\alignauthor Matthias Krug\\
      0828965\\
       \email{}
% 3rd. author
\alignauthor Stefanie Plieschnegger\\
      0926102\\
\and % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor Christian Proske\\
         1328245 \\
       \email{}
% 5th. author
\alignauthor Mino Sharkhawy \\
      1025887 \\
       \email{}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.

% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Sentiment analysis has become very popular in recent years and especially
Twitter provides a lot of data to a huge amount of topics which can be
processed and classified to provide an overall opinion. However, classification
of Twitter-based data is somehow different to traditional text mining and
introduce some additional challenges. In this paper the typical steps and problems of classifying tweets are outlined including preprocessing steps, training, and evaluation.
\end{abstract}


% A category with the (minimum) three required fields
\category{H.3}{Information Systems}{Information Search and Retrieval}

\terms{Theory}

\keywords{Sentiment Analysis, Opinion Mining, Twitter, Classifier, Natural Language Processing}

\section{Introduction} The general opinion about a specific product or service
has certainly a great influence on its reputation. People often want to know
what others think about a special product they are willing to buy, about a new
movie, or about a hotel they are going to book. But also companies may
interested in its customers' opinions, politicians may wish to receive
feedback, or social organizations may have interest in an ongoing debate.
\cite{pak2010twitter}.  The world wide web provides many ways for people to
distribute their experiences and sentiments. Machine learning algorithms make
it easier to process and evaluate those sentiments and are therefore able to
provide an overall opinion to a certain topic. This kind of analyzing is called
sentiment analysis or opinion mining \cite{liu2010sentimentanalysis,
pang2008opinion}. Clearly, there are some challenges when assessing the opinion
of people, especially when classifying microblogging services like Twitter
\footnote{\url{http://twitter.com}}. The underlying paper gives an overview
about different sentiment analysis approaches and outlines special problems
related to the classification of microblogging services. These challenges are
discussed in section~\ref{analyzingdata}. In section~\ref{preprocessing} transformations of tweets and feature selection are described. \ref{classification} deals with training and evaluation of classifiers. In section~\ref{related} some related work is introduced
and section~\ref{conclusion} concludes the paper.


\section{Analyzing Data} \label{analyzingdata} The analyzing process of text
and the classification whether its content is rather positive, negative or may
be considered as neutral, is the core functionality in sentiment analysis. We
are going to discuss general considerations of classifying data,
before outlining additional challenges related to Twitter-based data.

\subsection{Text Mining vs. Sentiment Analysis} One would assume, that text
mining and sentiment analysis are very similar. Text mining e.g. may deal with
classifying documents by topic, which represents one of the easier tasks.
Topic-based text classification tries to match a text into a category like
sport, politics etc. and therefore topic-related words are identified to
classify a text. However, sentiment analysis requires to focus on typical
"sentiment words" for example hate, love, like, regret.  When it comes to
identify the overall sentiment of a text it turns out that it may contains
several aspects (e.g. negative and positiver remarks) or may not even contain
any signal word but still has a negative
meaning.  To underline this problem, here are some examples:

\textit{"My new smartphone is really cool, the display is just gorgeous. The
battery life is really bad, however."}

\textsl{"Oh, of course - I have a lot of time. Just keep on using my money for
paying those really fast and friendly authorities."}

Obviously, this hardens the task of sentiment analysis. Moreover, it is hard to
teach a machine patterns like sarcasm or how to identify the intended meaning
behind words.  \cite{liu2010sentimentanalysis,pang2008opinion}


Another crucial point are dependencies of sentiments: topic, domain, and
temporal dependency. Those mainly focus on the problem, that sentiments can
have a different meaning, depending on the underlying topic or domain. E.g. a
word such as "unpredictable" may have a positive meaning if it used for movie
review, but could have a negative sentiment if it used to describe the behavior
of a car. Temporal dependency describes the problem of training a classifier with data from a
certain time-period and using this one for data classification of another time-period.
Those dependencies may have an influence on the accuracy of classification as well.
\cite{read2005using,pang2008opinion}

\subsection{Twitter-based Data} \label{twitter-based} 
Twitter is a form of microblog, where users can
post small text immediately and the so called tweets are contain real-time
reactions to certain events. The social network platform is categorized as
microblog as every tweet is limited to 140 signs. This results in people using
e.g. abbreviations, emoticons, slang and (intentional) spelling mistakes in order to fit
and express their opinion accurate. For example users sometimes try to put more emphasis on words by writing those in uppercase or repeating vocals (e.g. "I'm feeling happyyyyy"). Moreover, Twitter uses some special
characters like the \emph{@} (so-called target) which indicates that the post is directed to another
user. In addition hashtags are used to refer to special topics.  Another problem
is that Twitter data may contain spam. The fact, that tweets can be retweeted should also be kept in mind and may have an influence on the classifier, depending on the strategy of classification.
All these special characteristics play a
huge role when analyzing tweets. \cite{agarwal2011sentiment, read2005using}


\section{Preprocessing Tweets} \label{preprocessing} In order to achieve the
most precise result when classifying tweets, some preprocessing steps are
suggested. Preprocessing describes the cleansing of data and the text preparation for classification. It need to be remarked, that not all steps are necessary and it may also depends on the data set and the selected classifier how much influence these preprocessing tasks have. 
Generally, preprocessing contains data-cleansing steps, so called \texttt{transformations} and feature selection, also called \texttt{filtering} \cite{haddi2013therole}.


\subsection{Transformation}
Transformation contains all steps that make the data easier to classify, e.g. stripping whitespace, normalization (e.g. to lower case the text), methods for dealing with noisy data etc. 
In the following some of these approaches mentioned in \cite{ting2011naive, pak2010twitter, go2009twitter, agarwal2011sentiment, pang2008opinion, haddi2013therole} are shortly described.

\subsubsection{Extract Noisy Data}
The extraction of noisy data includes getting rid of e.g. advertisement/spam.

\subsubsection{Stopword Removal}
One of the most popular preprocessing steps is stopword filtering. Stopwords are defined as words that do not contain additional sentimental information and can therefore be removed from the text, as it will only make the text shorter, but does not lead to information loss. Such stopwords are for example: a, the, about, is, ...

\subsubsection{Stemming}
This approach deals with the identification of words that have a similar or identical meaning but are not spelled the same due to grammatically reasons, e.g. identifying "was" as a form of "be".

\subsubsection{Emoticon Dictionary}
Emoticons in tweets may be replaced with its actual meaning. This approach requires a list with all emoticons and its interpretation. Then those could be labeled e.g. according classifications like extremely-positive, positive, neutral, negative, extremely-negative as suggested in \cite{agarwal2011sentiment}.

\subsubsection{Stripping Emoticons}
However, the approach of stripping out emoticons of the training data has also been suggested: \cite{go2009twitter} consider emoticons as noisy data and experienced a better performance for training maximum entropy modeling (MaxEnt) and support vector machine (SVM) classifiers, although the test data may include emoticons.

\subsubsection{Acronym Dictionary}
This preprocessing approach deals with the use of likely abbreviations in Twitter-data. 
Typical acronyms in tweets are e.g. "lol" (laugh out loud), "brb" (be right back), "gr8" (great) etc.

\subsubsection{Replacing URLs and Targets}
Another preprocessing approach is replacing all URLs in tweets with special tag - this way the actual URL will not have an influence when classifying the data, only the fact that there is a URL will have an impact.
The same can be done with targets (already mentioned in section~\ref{twitter-based}).

\subsubsection{Replace Negations}
The replacement of all negative words like "non" or "never" by a tag "not" ease the classification as well.

\subsubsection{Replacing Repeated Characters}
As pointed out in section~\ref{twitter-based}, some people use repeated letters in words to put more weight on it. In order to make these words comparable it is tried to "normalize" them by replacing all characters that are repeated more than two (suggested in \cite{go2009twitter}) or more than three times (suggested in \cite{agarwal2011sentiment}). So a word like "happyyyyy" would become "happyy" (respectively "happyyy" if using the three-times-replacing approach). The strategy of replacing sequences by three characters makes the use of emphasized and normal words distinguishable.

\subsection{Feature Selection}
Features are those words or phrases of a text that will be selected to train a classifier and are  supposed to carry more sentiment or provide additional information. So the frequency of words may have an influence, or opinion words. Some approaches are described in the following.

\subsubsection{Tokenization and N-grams}
The data needs to be separated in order to use the words as features. Normally, the text is split by spaces and punctuation marks. In addition there are approaches to keep words like "don't" as one word \cite{pak2010twitter}. Tokenized words are also known as unigrams.
Using n-grams means that combinations of words are used. Unigrams are therefore combined, depending on the \texttt{n}. Approaches include e.g. combining unigrams and bigrams as features. 
\cite{liu2010sentimentanalysis, go2009twitter}

\subsubsection{Part of Speech}
Part of speech (POS) tags deal with the syntactic analysis of sentences. E.g. adjectives are assumed to provide sentimental meaning. \cite{liu2010sentimentanalysis} As the nature of tweets, POS taggers for Twitter need adaption, one approach can be found in \cite{gimpel2011part}. 

\subsubsection{Opinion Words}
Some words are known to carry special sentiment, e.g. love, hate, beautiful, great. Also phrases e.g. \texttt{"All that glitters isn't gold."} may be used for feature selection. \cite{liu2010sentimentanalysis}

\subsubsection{Twitter Specifics}
Obviously URLs, emoticons, hashtags and targets are carrying special meaning in each tweet and are therefore also considered as appropriate features. \cite{gimpel2011part}

\section{Classifying Tweets} \label{classification}
Twitter has become to a popular resource for sentiment analysis, as it provides a REST API \footnote{\url{https://dev.twitter.com/docs/api/1.1}} and a stream API \footnote{\url{https://dev.twitter.com/docs/streaming-apis}} to retrieve tweets
and therefore makes the collection of data easy. 
In this section the basic approach of training and testing classifiers will be outlined.


\subsection{Data Set} The corpus is the starting point of each sentiment
analyses. It contains the data the will be used to train a classifier and therefore should contain appropriate information. For supervised machine learning approaches, the data set needs to be labeled. There are a lot of data sets available as outlined in \cite{kouloumpis2011twitter}, however it is also possible to collect own data like suggested in \cite{pak2010twitter}.


\subsection{Training}
Classifiers are distinguished between unsupervised and supervised algorithms. Unsupervised ones e.g. rely on signal words and typical phrases and it mainly relies on pattern of POS tags. See \cite{liu2010sentimentanalysis} and \cite{pang2008opinion} for more details.

However, most sentiment classifications are based on supervised learning which requires a labeled data set (e.g. positive and negative), which will be separated into a training set and a testing set. Among the most popular classifiers in sentiment analysis are naive bayes and SVM. 

Naive bayes is a simple algorithm, that generally performs well in sentiment analysis domains. It calculates the likeliness that one object belongs to a class. It has been shown that preprocessing and feature selection play an important role in order to improve the accuracy of naive bayes.
\cite{ye2009sentiment, ting2011naive}

SVM usually perform better than naive bayes. Its approach is to separate the positive and negative training vectors of the data set with a maximum margin  \cite{ye2009sentiment}.

\subsection{Experiments and Evaluation}
The evaluation of classifiers is the same as for traditional machine learning algorithms. In order to find the best classifier, the preprocessing steps described in section~\ref{preprocessing} usually are used in different combinations. Moreover, k-fold-cross-validations is a common approach where data is split into k folds, using k-1 folds as training data and 1 as testing set, repeated k-times so that each fold will be used as testing set once. 

Typically the most important benchmark figures are accuracy, precision, recall, and F-measure. A huge role play the number of true positives (TP), true negatives (TN), as well as false negatives (FN) and false positive (FP) recognized items. 
\cite{haddi2013therole, ting2011naive, sokolova2006beyond, pak2010twitter, ye2009sentiment}


\begin{equation}
Accuracy = \frac{TP+TN}{TP+TN+FN+FP}
\end{equation}

\begin{equation}
Precision = \frac{TP}{TP+FP}
\end{equation}

\begin{equation}
Recall = \frac{TP}{TP+FN}
\end{equation}

\begin{equation}
F-measure = \frac{2*Recall*Precision}{Recall+Precision}
\end{equation}


\section{Related Work} \label{related}
The sentiment analysis of Twitter-data has been focused by various researchers. One of the results, described in \cite{agarwal2011sentiment} claims to have an average accuracy of around 60-75~\% (depending on the selected features and labels). 
The approach of testing different machine learning algorithms (naive bayes, MaxEnt, and SVM) combined with different features in \cite{go2009twitter} revealed an average accuracy of 80~\%, however. This is underlined by the case study in \cite{lin2012large}, which experienced a similar result.

\section{Conclusion} \label{conclusion}
When analyzing tweets specific characteristics like the limited size, slang, hashtags, targets etc. must be considered. We have listed typical preprocessing steps for Twitter data and provided an overview of classification approaches. 
As Twitter provides free access to its data and people and organizations keep interested in aggregated opinions, it is likely that sentiment analysis of tweets will become an even more popular research area in recent years.
%\end{document}  % This is where a 'short' article might terminate



%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

\end{document}
